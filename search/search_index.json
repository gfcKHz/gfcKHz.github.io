{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"gfcKHz","text":"<p>Field notes for metal-sdr. Posts track validation steps, derivations, and corrections that would otherwise disappear into commit messages.</p> <pre><code>pip install -r requirements-docs.txt\nmkdocs serve\n</code></pre>"},{"location":"TESTING/","title":"Testing Guide","text":""},{"location":"TESTING/#hardware-abstraction-layer-testing","title":"Hardware Abstraction Layer Testing","text":"<p>Verify that the hardware abstraction layer works correctly across backends and handles edge cases.</p>"},{"location":"TESTING/#phase-1-basic-functionality","title":"Phase 1: Basic Functionality","text":""},{"location":"TESTING/#test-1-backend-listing","title":"Test 1: Backend Listing","text":"<pre><code>cd scripts/capture\npython batch_capture.py --list-backends\n</code></pre> <p>Expected output: <pre><code>Available SDR backends:\n  rtl-sdr: available\n  bladerf: unavailable: bladeRF Python library not installed\n</code></pre></p> <p>Pass criteria: - Shows rtl-sdr as available - Shows bladerf as unavailable (until hardware acquired) - No errors or exceptions</p>"},{"location":"TESTING/#test-2-rtl-sdr-capture","title":"Test 2: RTL-SDR Capture","text":"<pre><code>python batch_capture.py --backend rtl-sdr --freq 105.9e6 --num-captures 1\n</code></pre> <p>Pass criteria: - Capture completes successfully - Creates <code>.sigmf-data</code> and <code>.sigmf-meta</code> files in <code>data/captures/</code> - Logs entry to SQLite database - Console shows:   - <code>[RTL-SDR] Command: rtl_sdr -f ...</code>   - <code>[RTL-SDR] Captured X bytes</code>   - <code>[RTL-SDR] Converted Y complex samples</code>   - <code>Capture 1 complete: capture_YYYYMMDD_HHMMSS.sigmf-data (ID: N)</code></p> <p>Verification: <pre><code># Check files created\nls -lh ../../data/captures/*.sigmf-*\n\n# Check database entry\nsqlite3 ../../data/captures/capture_manifest.db \\\n  \"SELECT id, center_freq_hz, sample_rate_hz, file_path FROM captures ORDER BY id DESC LIMIT 1;\"\n</code></pre></p>"},{"location":"TESTING/#test-3-bladerf-error-handling","title":"Test 3: BladeRF Error Handling","text":"<pre><code>python batch_capture.py --backend bladerf --freq 2.4e9 --num-captures 1\n</code></pre> <p>Expected output: <pre><code>ERROR: bladeRF Python library not installed. Install with: pip install bladerf\n\nAvailable backends:\n  rtl-sdr: available\n  bladerf: unavailable: bladeRF Python library not installed\n</code></pre></p> <p>Pass criteria: - Raises ImportError with helpful installation message - Lists available backends - Exits gracefully (no crash)</p>"},{"location":"TESTING/#phase-2-edge-cases","title":"Phase 2: Edge Cases","text":""},{"location":"TESTING/#test-4-invalid-backend","title":"Test 4: Invalid Backend","text":"<pre><code>python batch_capture.py --backend invalid --freq 105.9e6\n</code></pre> <p>Expected output: <pre><code>ERROR: Unknown backend: invalid. Supported: rtl-sdr, bladerf\n\nAvailable backends:\n  rtl-sdr: available\n  bladerf: unavailable: ...\n</code></pre></p> <p>Pass criteria: - Shows clear error message - Lists supported backends - Exits gracefully</p>"},{"location":"TESTING/#test-5-out-of-range-frequency-rtl-sdr","title":"Test 5: Out of Range Frequency (RTL-SDR)","text":"<pre><code># Too high (RTL-SDR max is 1.766 GHz)\npython batch_capture.py --backend rtl-sdr --freq 10e9\n</code></pre> <p>Expected output: <pre><code>ERROR: Frequency 10000.0 MHz out of range for rtl-sdr\n       Supported range: 24.0 - 1766.0 MHz\n</code></pre></p> <p>Pass criteria: - Catches frequency out of range before attempting capture - Shows valid frequency range for backend - Exits gracefully</p>"},{"location":"TESTING/#test-6-backward-compatibility","title":"Test 6: Backward Compatibility","text":"<p>Test existing captures still work:</p> <pre><code># Process old captures with fingerprinting\ncd ../fingerprinting\npython process_fingerprints.py\n</code></pre> <p>Pass criteria: - Old .sigmf-data files load successfully - Fingerprinting extracts features without errors - Database queries work unchanged</p> <p>Test database schema:</p> <pre><code>sqlite3 ../../data/captures/capture_manifest.db \\\n  \".schema captures\"\n</code></pre> <p>Pass criteria: - Schema unchanged from before refactor - All columns present: id, timestamp, center_freq_hz, sample_rate_hz, etc.</p>"},{"location":"TESTING/#phase-3-integration-testing","title":"Phase 3: Integration Testing","text":""},{"location":"TESTING/#test-7-end-to-end-pipeline","title":"Test 7: End-to-End Pipeline","text":"<pre><code># Capture \u2192 Process \u2192 Fingerprint\ncd scripts/capture\npython batch_capture.py --backend rtl-sdr --freq 105.9e6 --num-captures 1\n\ncd ../fingerprinting\npython process_fingerprints.py\n</code></pre> <p>Pass criteria: - Capture creates .sigmf-data file - Database logs capture - Fingerprinting extracts features - Database contains fingerprint entry</p> <p>Verification: <pre><code>sqlite3 ../../data/captures/capture_manifest.db &lt;&lt;EOF\nSELECT\n  c.id,\n  c.center_freq_hz/1e6 as freq_mhz,\n  f.peak_freq_hz/1e6 as peak_mhz,\n  f.cnr_db\nFROM captures c\nLEFT JOIN fingerprints f ON c.id = f.capture_id\nORDER BY c.id DESC\nLIMIT 1;\nEOF\n</code></pre></p> <p>Expected: Shows capture with associated fingerprint</p>"},{"location":"TESTING/#phase-4-stress-testing","title":"Phase 4: Stress Testing","text":""},{"location":"TESTING/#test-8-multi-capture-batch","title":"Test 8: Multi-Capture Batch","text":"<pre><code># Small batch to verify stability\npython batch_capture.py \\\n  --backend rtl-sdr \\\n  --freq 105.9e6 \\\n  --num-captures 5 \\\n  --interval 10\n</code></pre> <p>Pass criteria: - All 5 captures complete successfully - No memory leaks (monitor with <code>top</code> or <code>htop</code>) - Temp files cleaned up after each capture - Database contains 5 new entries</p>"},{"location":"TESTING/#test-summary-checklist","title":"Test Summary Checklist","text":"<ul> <li> Backend listing shows correct availability</li> <li> RTL-SDR capture works (creates files, logs to DB)</li> <li> BladeRF raises helpful error (until hardware available)</li> <li> Invalid backend shows clear error message</li> <li> Out-of-range frequency caught before capture</li> <li> Old captures still work with fingerprinting</li> <li> Database schema unchanged</li> <li> End-to-end pipeline works (capture \u2192 fingerprint)</li> <li> Multi-capture batch completes successfully</li> <li> No memory leaks or temp file buildup</li> </ul>"},{"location":"TESTING/#regression-testing","title":"Regression Testing","text":"<p>Before any commit that modifies capture code, run:</p> <pre><code># Quick regression test\ncd scripts/capture\npython batch_capture.py --list-backends\npython batch_capture.py --backend rtl-sdr --freq 105.9e6 --num-captures 1\n\ncd ../fingerprinting\npython process_fingerprints.py --file \"../../data/captures/capture_*.sigmf-data\"\n</code></pre> <p>All tests should pass with no errors.</p>"},{"location":"USAGE/","title":"Usage Guide","text":""},{"location":"USAGE/#hardware-abstraction-layer","title":"Hardware Abstraction Layer","text":"<p>The capture system supports multiple SDR backends through a common interface. Switch between hardware using the <code>--backend</code> flag.</p>"},{"location":"USAGE/#listing-available-backends","title":"Listing Available Backends","text":"<pre><code>cd scripts/capture\npython batch_capture.py --list-backends\n</code></pre> <p>Output: <pre><code>Available SDR backends:\n  rtl-sdr: available\n  bladerf: unavailable: bladeRF Python library not installed\n</code></pre></p>"},{"location":"USAGE/#rtl-sdr-usage","title":"RTL-SDR Usage","text":""},{"location":"USAGE/#single-capture","title":"Single Capture","text":"<pre><code>python batch_capture.py \\\n    --backend rtl-sdr \\\n    --freq 105.9e6 \\\n    --sample-rate 2.4e6 \\\n    --duration 3 \\\n    --gain 20 \\\n    --num-captures 1\n</code></pre>"},{"location":"USAGE/#batch-capture","title":"Batch Capture","text":"<pre><code># 10 captures, 5 minute intervals\npython batch_capture.py \\\n    --backend rtl-sdr \\\n    --freq 105.9e6 \\\n    --num-captures 10 \\\n    --interval 300\n</code></pre>"},{"location":"USAGE/#common-rtl-sdr-frequencies","title":"Common RTL-SDR Frequencies","text":"Signal Type Frequency Command FM Broadcast 88-108 MHz <code>--freq 105.9e6</code> ADS-B Aircraft 1090 MHz <code>--freq 1090e6</code> LoRa ISM (US) 915 MHz <code>--freq 915e6</code> Pagers 929 MHz <code>--freq 929e6</code>"},{"location":"USAGE/#bladerf-usage-future","title":"BladeRF Usage (Future)","text":""},{"location":"USAGE/#lte-capture","title":"LTE Capture","text":"<pre><code>python batch_capture.py \\\n    --backend bladerf \\\n    --freq 1.8e9 \\\n    --sample-rate 20e6 \\\n    --duration 5 \\\n    --gain 30\n</code></pre>"},{"location":"USAGE/#wifi-24-ghz-capture","title":"WiFi 2.4 GHz Capture","text":"<pre><code>python batch_capture.py \\\n    --backend bladerf \\\n    --freq 2.437e9 \\\n    --sample-rate 40e6 \\\n    --duration 2\n</code></pre>"},{"location":"USAGE/#wifi-5-ghz-capture","title":"WiFi 5 GHz Capture","text":"<pre><code>python batch_capture.py \\\n    --backend bladerf \\\n    --freq 5.18e9 \\\n    --sample-rate 40e6 \\\n    --duration 2\n</code></pre>"},{"location":"USAGE/#command-reference","title":"Command Reference","text":""},{"location":"USAGE/#required-arguments","title":"Required Arguments","text":"Flag Description Example <code>--freq</code> Center frequency (Hz) <code>105.9e6</code>"},{"location":"USAGE/#optional-arguments","title":"Optional Arguments","text":"Flag Default Description <code>--backend</code> <code>rtl-sdr</code> SDR hardware to use <code>--sample-rate</code> <code>2.4e6</code> Sample rate (Hz) <code>--duration</code> <code>3</code> Capture duration (seconds) <code>--gain</code> <code>20</code> Gain (dB) <code>--num-captures</code> <code>10</code> Number of captures <code>--interval</code> <code>300</code> Interval between captures (seconds) <code>--list-backends</code> - List available backends and exit"},{"location":"posts/capture/","title":"Ghost in the Capture","text":""},{"location":"posts/capture/#metal_sdr","title":"Metal_SDR","text":"<p>A measurement tool for RF captures. SDR feeds SigMF logs, fingerprints extract deterministic metrics (peak interpolation, CNR, bandwidth, adjacent-channel rejection), and regression tests keep the math anchored to physics rather than intention. The sections below walk through the failure modes uncovered while building that pipeline and the corrections layered in so each capture carries defensible results.</p>"},{"location":"posts/capture/#field-journal","title":"Field Journal","text":"<p>Why FM broadcast? It\u2019s a stable, public signal I can listen to, analyze and validate within an acceptable amount of error based on my hardware constraints. Stations occupy ~200\u202fkHz and have published parameters for stress testing the capture loop before facing OFDM systems.</p>"},{"location":"posts/capture/#wide-bandwidth-source-ambiguity","title":"Wide-bandwidth Source Ambiguity","text":"<ul> <li>Setup: RTL-SDR @ 2.4\u202fMsps sweeping FM broadcast from 100\u2013142\u202fMHz.  </li> <li>Symptom: metadata said \u201c105.9\u202fMHz,\u201d but the stored spectrum often contained 105.4 or 106.3\u202fMHz whenever those stations were louder.  </li> <li>Lesson: tuner settings document intent. Without a validation step, a wide window silently captures whichever emitter dominates the moment.</li> </ul> <p>Course correction: narrow each capture window around the target station; log the measured peak frequency (post-capture) instead of trusting the tuner.</p>"},{"location":"posts/capture/#single-feature-validation-fallacy","title":"Single-feature Validation fallacy","text":"<ul> <li>Setup: narrowed windows but still used \u201cmaximum FFT bin\u201d as the only check.  </li> <li>Symptom: station IDs drifted \u00b10.5\u202fMHz as the dongle\u2019s oscillator warmed up; adjacent-channel interference triggered new entries in the manifest.  </li> <li>Lesson: a single feature can\u2019t tell \u201ctuner drift\u201d from \u201cnew signal.\u201d Multi-feature fingerprints (peak interpolation, CNR, bandwidth, adjacent rejection, roll-off) are mandatory.</li> </ul> <p>The multi-feature fix lives in (<code>fm_fingerprint.py</code>): takes each capture and emits a structured feature vector (peak frequency, CNR, 3\u202fdB bandwidth, adjacent-channel rejection, rolloff asymmetry). Even the \u201cfind the peak\u201d step is formalized with a three-point parabolic interpolation,</p> \\[ \\large \\delta = \\tfrac{1}{2}\\,\\frac{P_{k-1}-P_{k+1}}{P_{k-1}-2P_k+P_{k+1}}, \\qquad f_{\\text{peak}} = f_k + \\delta\\,\\Delta f, \\] <p>where \\(P_i\\) are log-PSD samples and \\(\\Delta f\\) is the FFT bin spacing (scripts/fingerprinting). That keeps every capture anchored to the same peak-estimation math instead of trusting one noisy FFT bin.</p>"},{"location":"posts/capture/#carrier-to-noise-ratio-needs-dimensional-consistency","title":"Carrier-to-Noise ratio needs dimensional consistency","text":"<p>Identical night captures reported CNR swings between 50-72\u202fdB because the numerator summed carrier PSD bins \\(\\left(\\sum_{k\\in\\mathcal{C}} S[k]\\right)\\) while the denominator used average noise PSD \\(\\left(|\\mathcal{N}|^{-1} \\sum_{k\\in\\mathcal{N}} S[k]\\right)\\). That mismatch silently injects \u224822\u202fdB of phantom CNR by counting \\(|\\mathcal{C}|\\) extra bins. The correct form compares mean PSD on both sides:</p> \\[ \\bar{S}_{\\text{carrier}} = \\frac{1}{|\\mathcal{C}|} \\sum_{k \\in \\mathcal{C}} S[k], \\qquad \\bar{S}_{\\text{noise}} = \\frac{1}{|\\mathcal{N}|} \\sum_{k \\in \\mathcal{N}} S[k], \\qquad \\mathrm{CNR}_{\\text{correct}} = 10 \\log_{10} \\left( \\frac{\\bar{S}_{\\text{carrier}}}{\\bar{S}_{\\text{noise}}} \\right) \\] <p>Fix: compare mean PSD on both sides:</p> <pre><code># scripts/fingerprinting/fm_fingerprint.py:118\ncarrier_mask = np.abs(freqs - peak_freq_hz) &lt;= carrier_bw_hz\ncarrier_power_density = np.mean(psd[carrier_mask])\n\nnoise_mask = np.abs(freqs - peak_freq_hz) &gt; guard_bw_hz\nnoise_bins = psd[noise_mask]\nnoise_floor = np.mean(np.partition(noise_bins, kth)[:n_bins])\n\ncnr_db = 10 * np.log10(carrier_power_density / noise_floor)\n</code></pre> <p>Both sides stay as mean PSD:</p> <p>The corrected ratio stays in mean-PSD land: carrier and noise are averaged over matching bandwidths before taking \\(10\\log_{10}\\) of their quotient.</p>"},{"location":"posts/capture/#welch-psd-requires-bin-width-scaling","title":"Welch PSD requires bin-width scaling","text":"<p><code>signal.welch(..., scaling='density')</code> returns PSD values \\(S[k]\\) in \\([\\text{W/Hz}]\\). Summing those values without multiplying by the frequency bin width \\(\\Delta f\\) makes results depend on FFT length. The conversion is straightforward:</p> \\[ P_{\\text{carrier}} = \\left( \\sum_{k \\in \\mathcal{C}} S[k] \\right) \\Delta f, \\qquad \\Delta f = f[k+1] - f[k] \\] <p>All metrics that integrate PSD now multiply by <code>freq_resolution</code>; ratios stay between averaged densities.</p>"},{"location":"posts/capture/#adjacent-channel-rejection-needs-matching-bandwidths","title":"Adjacent-channel rejection needs matching bandwidths","text":"<p>Same pitfall as CNR: the original code integrated carrier power but compared it against the average adjacent-channel power. Averaging PSD over identical \u00b150\u202fkHz windows fixes the mismatch:</p> <pre><code># scripts/fingerprinting/fm_fingerprint.py:205\ncarrier_mask = np.abs(freqs - peak_freq_hz) &lt;= 50e3\ncarrier_power_density = np.mean(psd[carrier_mask])\n\nleft_center = peak_freq_hz - channel_spacing_hz\nleft_mask = np.abs(freqs - left_center) &lt;= 50e3\nleft_power_density = np.mean(psd[left_mask])\n\nright_center = peak_freq_hz + channel_spacing_hz\nright_mask = np.abs(freqs - right_center) &lt;= 50e3\nright_power_density = np.mean(psd[right_mask])\n\nrejection_db = 10 * np.log10(\n    carrier_power_density / np.maximum(1e-15, (left_power_density + right_power_density) / 2.0)\n)\n</code></pre>"},{"location":"posts/capture/#rtl-sdr-normalization-must-be-symmetric","title":"RTL-SDR normalization must be symmetric","text":"<p>Dividing raw samples by 127.5 placed zero halfway between two quantization codes. Dividing by 128.0 centers zero exactly, removing a deterministic DC bias:</p> <pre><code># scripts/capture/backends/rtl_sdr.py:95\nraw = np.fromfile(temp_path, dtype=np.uint8)\niq_float = (raw.astype(np.float32) - 127.5) / 128.0\niq = iq_float[0::2] + 1j * iq_float[1::2]\n</code></pre>"},{"location":"posts/capture/#testing","title":"Testing","text":"<p>Once the signal chain math was honest, I still needed proof. The regression suite generates synthetic IQ blocks with known SNR/frequency to ensure the extractor reports what physics says it should:</p> <pre><code>def test_cnr_with_known_signal():\n    fs = 2.4e6\n    duration = 0.1\n    t = np.arange(0, duration, 1/fs)\n\n    signal_power = 1.0\n    carrier = np.exp(2j * np.pi * 0 * t) * np.sqrt(signal_power)\n\n    target_snr_db = 30.0\n    noise_power = signal_power / (10**(target_snr_db / 10))\n    noise = (np.random.randn(len(t)) + 1j * np.random.randn(len(t))) * np.sqrt(noise_power / 2)\n\n    iq = carrier + noise\n    features = extract_fingerprint(iq.astype(np.complex64), fs, 0.0)\n    assert abs(features[\"cnr_db\"] - target_snr_db) &lt; 2.0\n</code></pre> <p>There\u2019s a companion test for frequency accuracy (parabolic interpolation within 100\u202fHz). Together they act as the \u201canalysis\u201d block\u2019s tripwire: any change to the fingerprint code reruns the synthetic suite before merging.</p>"},{"location":"posts/capture/#what-this-enables","title":"What This Enables","text":"<p>Propagation tracking: does WQXR's carrier drift 200 Hz warmer at 2 PM than at 2 AM? Does atmospheric ducting at night pull in stations from 300+ miles away? Multi-day captures with sub-100 Hz peak interpolation and timestamped CNR logs answer questions that spectrum analyzer screenshots cannot.</p> <p>Interference detection: when CNR drops 8 dB between successive captures, or when adjacent-channel rejection degrades from 40 dB to 18 dB, the pipeline flags it. Cross-reference against weather data, time of day, and nearby capture sessions to distinguish atmospheric fade from a new emitter appearing in-band.</p> <p>Relational grounding for ML: models hallucinate, just like people do. None of us interface with reality directly. We experience it through peripherals, sensors, and neurophysiological smoothing functions that hide thermal jitter and hardware drift. Nobuo reminds us that we understand an object only through its relationships. There is no true representation, only the web of all possible representations. Whatever we believe to be the intrinsic meaning of an object is just a useful abstraction, true meaning lies between networks of relations between different objects. A SigMF capture is meaningful when the relationships it claims (frequency, CNR, bandwidth) match the measurable traces coming off the radio. Validation keeps those morphisms honest so the model\u2019s hallucinations stay tethered to the same perceptual field the hardware saw.</p> <p>Foundation: FM broadcast is the introduction. If the same CNR checks, bandwidth validation, and adjacent-channel rejection metrics can scale to LTE downlink (15 kHz subcarriers, 20 MHz channels), WiFi OFDM (312.5 kHz subcarriers, 20/40/80 MHz channels), and LoRa chirps (125\u2013500 kHz), then the bladeRF backend brings the same regression harness.</p>"},{"location":"posts/capture/#references","title":"References","text":"<ol> <li>Gardner, William A. Statistical Spectral Analysis: A Nonprobabilistic Theory. Prentice Hall, 1987. [Cyclostationary coherence framework underpinning the validation heuristics]</li> <li>Gardner, William A. \"A Unifying View of Coherence in Signal Processing.\" Signal Processing 29 (1992): 113\u2013140.</li> <li>Spooner, Chad. \"Correcting the Record: Comments on Wireless Signal Representation Techniques for Automatic Modulation Classification.\" (2022).</li> <li>Bradley, Tai-Danae. \"The Yoneda Perspective.\" Math3ma, May 2017. https://www.math3ma.com/blog/the-yoneda-perspective. [Intuition primer that motivates the Yoneda link in plain language]</li> <li>Wei, Haoran, Yaofeng Sun, and Yukun Li. \"DeepSeek-OCR: Contexts Optical Compression.\" arXiv:2510.18234 (2025). [Optical context compression for high-accuracy OCR with constrained vision tokens]</li> <li>Shao, Chenze, Darren Li, Fandong Meng, and Jie Zhou. \"Continuous Autoregressive Language Models.\" arXiv:2510.27688 (2025). [Continuous next-vector generation to increase semantic bandwidth per decoding step]</li> <li>O'Shea, Timothy J., Johnathan Corgan, and T. Charles Clancy. \"Convolutional Radio Modulation Recognition Networks.\" arXiv:1602.04105 (2016). [CNN baselines for end-to-end modulation recognition]</li> <li>Peng, Shengliang, Hanyu Jiang, Huaxia Wang, Hathal Alwageed, Yu Zhou, Marjan Mazrouei Sebdani, and Yu-Dong Yao. \"Modulation Classification Based on Signal Constellation Diagrams and Deep Learning.\" IEEE Transactions on Neural Networks and Learning Systems 30, no. 3 (2019): 718\u2013727. https://doi.org/10.1109/TNNLS.2018.2850703. [Constellation image CNN that set the standard for diagram-first modulation tasks]</li> <li>Zhang, Hao, Fuhui Zhou, Hongyang Du, Qihui Wu, and Chau Yuen. \"Revolution of Wireless Signal Recognition for 6G: Recent Advances, Challenges and Future Directions.\" IEEE Communications Surveys &amp; Tutorials (2025). https://doi.org/10.1109/COMST.2025.3569427. [6G-era survey of modulation/fingerprint pipelines, open problems, and data requirements]</li> <li>Wolfram, Stephen. \"I Have a Theory Too: The Challenge and Opportunity of Avocational Science.\" August 2025. https://writings.stephenwolfram.com/2025/08/i-have-a-theory-too-the-challenge-and-opportunity-of-avocational-science/. [Framework for keeping hobby-grade theory accountable]</li> </ol>"},{"location":"posts/entropy/","title":"Ghost in the Capture \u2014 Entropy","text":""},{"location":"posts/entropy/#micro-fluctuation","title":"Micro-Fluctuation","text":"<p>Entropy is just instrumentation with variance we can audit. Frequency, jitter, and bias only count if they stay tethered to the hardware that emitted them, so every reduction pass doubles as a witness channel. Each capture leaves a meshed trail (carrier drift, mixer jitter, PLL recoveries) and those traces either count as entropy or expose the side channel. Every run emits the same fluctuation vector (drift, phase noise, temperature swing, supply ripple) and that packet rides with the data so any \u201cfresh randomness\u201d claim has to debit a specific line item. Persisting the trace lets us replay it through whitening, masking, or shuffling and watch the signature contract in real time. The log becomes the handshake between all these measurements; every knob turn is pinned to a readable signature instead of a hunch.</p>"},{"location":"posts/entropy/#local-instrumentation","title":"Local Instrumentation","text":"<p>Logic Pro\u2019s stem splitter rewrites the same story on the audio side. Because its ML pipeline (drums, bass, vocals, other) runs entirely on-device, every decomposition happens inside the same hardware envelope as the rest of the DAW (no external model, API call, or hosted inference path). Ableton followed the same path with Live 12.3\u2019s stem splitter, which means I get two independent local decompositions to compare. When Logic generates a texture that Ableton doesn\u2019t, that difference isn\u2019t random; it carries quantization noise, memory access cadence, and floating-point drift from each autoencoder. The machinery becomes a pair of witness channels whose fluctuations persist with the stems they emit. Local processing turns both tools into instruments with traceable variance: I get entropy by proxy because each model\u2019s variance is literally my hardware\u2019s variance, and the delta between them exposes which fingerprints are stable versus model-specific.</p>"},{"location":"posts/entropy/#bibliography","title":"Bibliography","text":"<ol> <li>Zeng, Kevin, and Roozbeh Bassirian. \"Practical Doppler Side Channels on Wireless Encryption.\" IEEE Transactions on Information Forensics and Security 19 (2024): 1123\u20131136.</li> <li>Holcomb, Daniel E., William P. Burleson, and Kevin Fu. \"Power-Up SRAM State as a True Random Number Generator.\" IEEE Journal of Solid-State Circuits 48, no. 1 (2013): 917\u2013930.</li> <li>Sunar, Berk, WeiJiaan Martin, and Daniel R. Stinson. \"A Provably Secure True Random Number Generator with Built-in Tolerance to Active Attacks.\" IEEE Transactions on Computers 56, no. 1 (2007): 109\u2013119.</li> <li>Tokunaga, Claudio, and David Blaauw. \"True Random Number Generator with a Metastability-Based Entropy Source.\" IEEE Journal of Solid-State Circuits 45, no. 1 (2010): 70\u201377.</li> <li>Kocher, Paul, Joshua Jaffe, and Benjamin Jun. \"Differential Power Analysis.\" In Advances in Cryptology \u2013 CRYPTO '99, 1999.</li> <li>Cagli, Emanuele, C\u00e9cile Dumas, and Emmanuel Prouff. \"Convolutional Neural Networks with Data Augmentation for Side-Channel Attacks.\" In Proceedings of CHES, 2017.</li> </ol>"},{"location":"posts/space/","title":"Ghost in the Capture \u2014 Latent Space","text":"<p>I grabbed my headphones (Sony WH-1000XM4) and listened to one of my favorite songs by On Broken Wings called The Spawning of Progression. After saving the track as a wav (via the sample chrome extension) and uploading it to a stem splitter (moises), the isolated bass revealed something unexpected. What initially seemed like a hidden tonal state (maybe a pitch inversion of the noise at the end of the song), turned out to be an artifact: a reconstruction (hallucinated) of the original sample by the autoencoder in the stem splitter. </p> <p>After listening to the whole song, there are wind chimes faintly present at the beginning that I think correspond to the sound I was hearing towards the end (with the bass isolated). Ultimately, this is important because this reframing of the source material rotates the perceptual modality we operate from by shifting not just what is heard in tonal space but where one is positioned relative to the source and its constituent decompositions, revealing different transformations.</p>"},{"location":"posts/space/#we-all-knew-this-was-coming","title":"We All Knew This Was Coming","text":"<p>So crazy thing I learned recently, was that most audio models don\u2019t actually operate on audio at all. There are some bespoke ones that do, but the majority convert the audio to a spectrogram then pass that spectrogram into an image2* model. Img2img for new audio, reconstruct the spectrogram with encodec at the output layer. I have also been playing with that, re-representing patches of tokens as a unified vector, representing that vector in a pixel group, with some masked pixels for pattern learning. - SashimiSaketoro</p> <p>\u201cLong-term, &gt;99% of input and output for AI models will be photons. Nothing else scales.\u201d - Elon Musk replying to Andrej Karpathy on DeepSeek-OCR</p> <p>So all the cool kids are beam splitting audio through pixel space and I'm late to the party as usual. This is when it naturally occurred to me that I haven't been paying attention to what's really going on (and it doesn't help that I don't even know where my glasses are right now). Perception is computationally bounded, and vision transforms once the perceptual map reshapes to the relational geometry that emerges from constraints. Maybe I'm just intercepting a broadcast from a CDMA downlink, crossing the boundary William carved out of coherence. </p>"},{"location":"posts/space/#johnsonlindenstrauss","title":"Johnson\u2013Lindenstrauss","text":"<p>The Johnson\u2013Lindenstrauss lemma shows that a projection can preserve relational structure even when the coordinate system changes. Formally, for any \\(0&lt;\\varepsilon&lt;1\\) and finite \\(X \\subset \\mathbb{R}^n\\) there exists \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k\\), \\(k = O(\\varepsilon^{-2} \\log |X|)\\), such that:</p> \\[ \\large (1-\\varepsilon)\\|x-y\\|_2^2 \\;\\le\\; \\|f(x)-f(y)\\|_2^2 \\;\\le\\; (1+\\varepsilon)\\|x-y\\|_2^2 \\quad \\forall x,y \\in X. \\] <p>The JL Lemma Beyond Euclidean Geometry talk at FWCG 2025 was interesting. Their extension to pseudo-Euclidean signatures and generalized power-distance matrices matches how my own session clouds behave once drift and noise whitening push them off a tidy \\(\\mathbb{R}^p\\) baseline. Imagine two channels occupying disjoint kernels (time scales). Take a Coltrane sample and encode it as a drift feature and a drum sequence rendered as a rhythmic probe. The bridge between those abstractions sits in the Gromov\u2013Hausdorff regime, where I stop comparing coordinates and instead compare the metric spaces themselves up to isometry. JL provides the distortion bounds that let me embed each channel into a common space without losing the pairwise distance structure and Gromov\u2013Hausdorff tells me whether differently instrumented spaces still settle into the same rhythm and coherence.  </p>"},{"location":"posts/space/#gromovhausdorff","title":"Gromov\u2013Hausdorff","text":"<p>M\u00e9moli\u2019s CIRM tutorial frames the Gromov\u2013Hausdorff distance by embedding both compact metric spaces into a \u201csufficiently rich\u201d ambient space \\((Z,d_Z)\\) and measuring the Hausdorff distance between the images:</p> \\[ \\large d_{GH}(X,Y) \\;=\\; \\inf_{Z,\\phi_X,\\phi_Y} d_{\\mathcal{H}}^{Z}\\!\\left(\\phi_X(X),\\,\\phi_Y(Y)\\right), \\] <p>where \\(\\phi_X : X \\rightarrow Z\\) and \\(\\phi_Y : Y \\rightarrow Z\\) range over all isometric embeddings.[14,15] This viewpoint emphasizes the search for a shared latent space where I can compare copies directly. Either way, the metric lets me compare latent clouds that arise from wildly different instrumentation. A high-fidelity sax stem drifting in a covariance-adapted \\(\\mathbb{R}^p\\) can sit next to a percussion stem traced through a symbolic proof tree: the GH distance ignores how these spaces are parameterized and instead tracks whether the relational loops (motifs, cadences, or tempo envelopes) survive. When I glue GH onto a JL pipeline, I flatten each rendering only as far as local distortions allow and then measure whether the rhythmic coherence still overlays in an isometry-class sense. A low \\(d_{GH}\\) tells me the two renderings encode the same musical ghost, even if one lives in accelerometer counts and the other in decision-logic kernels.  </p> <p>Practically, you build the bridge by sampling anchor phrases or phase-aligned downbeats, wiring them into a correspondence, and letting an optimal-transport or barycentric algorithm tighten the distortion.[16] The residual becomes an actionable \u201cmetric mismatch\u201d that can be pushed back into the signal chain: adjust whitening, reweigh drift features, or re-synchronize stems until the GH witness falls beneath a perceptual budget. GH therefore acts less like an abstract invariant and more like a control knob for keeping multiple sensing modalities faithful to the same improvisation.  </p>"},{"location":"posts/space/#heilbronn-triangulation","title":"Heilbronn Triangulation","text":"<p>A polar grid partitioned into twelve equal sectors (nodes pinned to orbital longitudes) gives a strict geometric surface that lines up with  the tonnetz\u2019s 12 pitch classes/24 triads. In that shared symmetry, sector indices map directly onto circle-of-fifths coordinates, so the workflow becomes a deterministic analogue of the Heilbronn triangulation problem: place \\(n\\) points inside a disk so every triangle determined by three points has area at least \\(c/n^2\\) for some constant \\(c\\). Brass packages the proof sketches into a discrete-geometry playbook that overlays nicely on twelve-tone lattices. The polar grid is the timestamped tiling I already use; Heilbronn\u2019s constraint forces any trio of simultaneous stems (horn, drums, strings) to span a triangle whose area lower bound blocks degeneracy. Once the triangle areas are bounded, JL can reduce the ambient dimension without collapsing the simplices, and GH can compare the resulting triangulations across sensing kernels. The grid is the control surface dictating how much simplex area each trio of stems must maintain during an improvisation.</p>"},{"location":"posts/space/#tonnetz-as-rotational-lattice","title":"Tonnetz as Rotational Lattice","text":"<p>Coltrane\u2019s circle of tones is a tonnetz in disguise (a 12-node rotational lattice where translations correspond to voice-leading moves). Neo-Riemannian theory packages those moves into the \\(P\\), \\(L\\), and \\(R\\) operators. Overlay the polar grid on the lattice and stacking stems becomes a coset-selection problem. The \u201crotational lattice\u201d phrase just means each channel walks that tonnetz, so coherence demands their paths align modulo the symmetries. Yoneda\u2019s lemma keeps the categorical metaphysics honest: chords, angular sectors, and stems only matter (or rather that the matter creating from them is derived) through their relations. JL stabilizes distances on the lattice, GH/GW checks whether two renderings (say a Coltrane stem and a Whiplash percussion overlay) trace isometric loops, and Heilbronn triangulation keeps the local simplices nondegenerate while the tonnetz preserves the global rotation matrix.</p> <p>The aim is to keep the rotational lattice auditable: a discrete engine for latent space where every combination of stems respects both the Heilbronn area budget and the tonnetz symmetries. Hardware micro-fluctuations (drift, jitter, PLL relock cadence) feed into the lattice as auxiliary coordinates, so the stem splitter behaves like an audio side-channel: every generated stem carries the user's geometric and hardware fingerprint.</p> crocodile  \u00b7  Shadow Tonnetz"},{"location":"posts/space/#references","title":"References","text":"<ol> <li>Elhage, Nelson, et al. \"Toy Models of Superposition.\" Transformer Circuits, 2022. https://transformer-circuits.pub/2022/toy_model/index.html.</li> <li>Anthropic. \"Linebreaks: Spatial Structure in Language Models.\" Transformer Circuits, 2025. https://transformer-circuits.pub/2025/linebreaks/index.html.</li> <li>Hirschberg, D. S. \"Algorithms for the Longest Common Subsequence Problem.\" Journal of the ACM 24, no. 4 (1977): 664\u2013675.</li> <li>Baraniuk, Richard G., and Mark A. Davenport. \"A Simple Proof of the Restricted Isometry Property for Random Matrices.\" Constructive Approximation 28, no. 3 (2008): 253\u2013263.</li> <li>Krahmer, Felix, and Rachel Ward. \"New and Improved Johnson\u2013Lindenstrauss Embeddings via the Restricted Isometry Property.\" SIAM Journal on Mathematical Analysis 43, no. 3 (2011): 1269\u20131281.</li> <li>Ailon, Nir, and Bernard Chazelle. \"The Fast Johnson\u2013Lindenstrauss Transform and Approximate Nearest Neighbors.\" SIAM Journal on Computing 39, no. 1 (2009): 302\u2013322.</li> <li>Baraniuk, Richard G., and Michael B. Wakin. \"Random Projections of Smooth Manifolds.\" Foundations of Computational Mathematics 9, no. 1 (2009): 51\u201377.</li> <li>Nelson, Jelani. \"Dimensionality Reduction.\" UC Berkeley lecture notes, August 1, 2022.</li> <li>Silwal, Sandeep. \"Beyond Worst-Case Dimensionality Reduction for Sparse Vectors.\" arXiv preprint, 2023.</li> <li>Brass, Peter. Research Problems in Discrete Geometry. Springer, 2005.</li> <li>Gilbert, Anna C., Martin J. Strauss, Joel A. Tropp, and Roman Vershynin. \"One Sketch for All: Fast Algorithms for Compressed Sensing.\" In Proceedings of the 39th Annual ACM Symposium on Theory of Computing, 2007.</li> <li>Deng, Chengyuan, Jie Gao, Kevin Lu, Feng Luo, and Cheng Xin. \"Johnson\u2013Lindenstrauss Lemma Beyond Euclidean Geometry.\" arXiv preprint arXiv:2310.22491, 2023.</li> <li>Lim, Lek-Heng, Tetsuya Maehara, and Ken N. Smith. \"Distortion Bounds for Spheres and the Borsuk\u2013Ulam Frontier.\" Research manuscript, 2022.</li> <li>Dubois, Michel, and Laurent Schwartz. \"Lower Bounds on Metric Distortion over Spheres.\" Annales de l'Institut Fourier 31, no. 3 (1981): 1\u201333.</li> <li>Burago, Dmitri, Yuri Burago, and Sergei Ivanov. A Course in Metric Geometry. American Mathematical Society, 2001.</li> <li>Gromov, Mikhail. Metric Structures for Riemannian and Non-Riemannian Spaces. Birkh\u00e4user, 1999.</li> <li>Sturm, Karl-Theodor. \"On the Geometry of Metric Measure Spaces.\" Acta Mathematica 196, no. 1 (2006): 65\u2013131.</li> <li>M\u00e9moli, Facundo. \"The Gromov\u2013Hausdorff Distance: A Brief Tutorial on Some of Its Quantitative Aspects.\" Actes des rencontres du C.I.R.M. 3, no. 1 (2013): 89\u201396.</li> <li>Popoff, Alexandre, Corentin Guichaoua, and Moreno Andreatta. \"Composing (with) Automorphisms in the Colored Cube Dance: An Interactive Tool for Musical Chord Transformation.\" Manuscript, 2022.</li> </ol>"}]}